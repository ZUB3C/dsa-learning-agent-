import argparse
import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path

from pydantic import BaseModel

from src.agents.registry import load_agent


class Question(BaseModel):
    question_id: int
    difficulty: str
    question_text: str
    expected_answer: str
    user_answer: str
    key_points: list[str]
    is_correct: bool  # Ground truth


class Topic(BaseModel):
    topic_id: str
    topic_name: str
    questions: list[Question]


class TestCollection(BaseModel):
    creation_date: str
    total_questions: int
    topics_count: int
    topics: list[Topic]


class PrimaryEvaluation(BaseModel):
    is_correct: bool
    feedback: str


class SecondaryEvaluation(BaseModel):
    agree_with_primary: bool
    is_correct: bool
    feedback: str
    verification_notes: str | None = None


class TestVerification(BaseModel):
    question_id: int
    topic: str
    difficulty: str
    ground_truth: bool
    primary_evaluation: PrimaryEvaluation
    secondary_evaluation: SecondaryEvaluation
    timestamp: str


class VerificationMetrics(BaseModel):
    total_verifications: int
    agreement_count: int
    disagreement_count: int
    agreement_rate: float
    # –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
    primary_accuracy: float
    secondary_accuracy: float
    improvement_rate: float


class EffectivenessReport(BaseModel):
    report_date: str
    overall_metrics: VerificationMetrics
    verifications: list[TestVerification]


async def verify_answer(
    question: Question, language: str = "ru"
) -> tuple[PrimaryEvaluation, SecondaryEvaluation]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ç–≤–µ—Ç –±–µ–∑ –ø–µ—Ä–µ–¥–∞—á–∏ is_correct"""
    try:
        # –ü–µ—Ä–≤–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        primary_agent = load_agent("verification", language=language)
        primary_result = await primary_agent.ainvoke({
            "question": question.question_text,
            "expected_answer": question.expected_answer,
            "user_answer": question.user_answer,
        })

        try:
            primary_eval_dict = json.loads(primary_result)
            primary_eval = PrimaryEvaluation(**primary_eval_dict)
        except (json.JSONDecodeError, ValueError):
            primary_eval = PrimaryEvaluation(is_correct=False, feedback="–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞")

        # –í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        secondary_agent = load_agent("verification-secondary", language=language)
        secondary_result = await secondary_agent.ainvoke({
            "primary_evaluation": json.dumps(primary_eval.model_dump(), ensure_ascii=False),
            "question": question.question_text,
            "user_answer": question.user_answer,
        })

        try:
            secondary_eval_dict = json.loads(secondary_result)
            secondary_eval = SecondaryEvaluation(**secondary_eval_dict)
        except (json.JSONDecodeError, ValueError):
            secondary_eval = SecondaryEvaluation(
                agree_with_primary=True,
                is_correct=primary_eval.is_correct,
                feedback=primary_eval.feedback,
                verification_notes="–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞",
            )

        return primary_eval, secondary_eval

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤ –≤–æ–ø—Ä–æ—Å–µ {question.question_id}: {e}")
        return (
            PrimaryEvaluation(is_correct=False, feedback="–û—à–∏–±–∫–∞"),
            SecondaryEvaluation(
                agree_with_primary=False,
                is_correct=False,
                feedback="–û—à–∏–±–∫–∞",
                verification_notes=str(e),
            ),
        )


async def process_verifications(
    test_collection: TestCollection, language: str = "ru"
) -> list[TestVerification]:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã"""
    verifications = []
    total = test_collection.total_questions
    processed = 0

    for topic in test_collection.topics:
        for question in topic.questions:
            processed += 1
            print(f"[{processed}/{total}] –í–æ–ø—Ä–æ—Å {question.question_id}: {topic.topic_name}")

            primary_eval, secondary_eval = await verify_answer(question, language)

            verification = TestVerification(
                question_id=question.question_id,
                topic=topic.topic_name,
                difficulty=question.difficulty,
                ground_truth=question.is_correct,
                primary_evaluation=primary_eval,
                secondary_evaluation=secondary_eval,
                timestamp=datetime.now().isoformat(),
            )

            verifications.append(verification)

    return verifications


def calculate_metrics(verifications: list[TestVerification]) -> VerificationMetrics:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –±–∞–ª–ª–æ–≤"""
    if not verifications:
        return VerificationMetrics(
            total_verifications=0,
            agreement_count=0,
            disagreement_count=0,
            agreement_rate=0.0,
            primary_accuracy=0.0,
            secondary_accuracy=0.0,
            improvement_rate=0.0,
        )

    total = len(verifications)
    agreements = sum(1 for v in verifications if v.secondary_evaluation.agree_with_primary)
    disagreements = total - agreements

    # –¢–æ—á–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–æ–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ ground truth
    primary_correct = sum(
        1 for v in verifications if v.primary_evaluation.is_correct == v.ground_truth
    )
    secondary_correct = sum(
        1 for v in verifications if v.secondary_evaluation.is_correct == v.ground_truth
    )

    primary_accuracy = (primary_correct / total) * 100 if total > 0 else 0
    secondary_accuracy = (secondary_correct / total) * 100 if total > 0 else 0
    improvement_rate = secondary_accuracy - primary_accuracy

    return VerificationMetrics(
        total_verifications=total,
        agreement_count=agreements,
        disagreement_count=disagreements,
        agreement_rate=(agreements / total * 100) if total > 0 else 0,
        primary_accuracy=primary_accuracy,
        secondary_accuracy=secondary_accuracy,
        improvement_rate=improvement_rate,
    )


def load_test_collection_from_file(filepath: str) -> TestCollection:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é"""
    with Path(filepath).open(encoding="utf-8") as f:
        data = json.load(f)

    topics = []
    for test in data.get("test_collection", {}).get("tests", []):
        questions = [Question(**q) for q in test.get("questions", [])]
        topics.append(
            Topic(
                topic_id=test.get("test_id", ""),
                topic_name=test.get("topic", ""),
                questions=questions,
            )
        )

    return TestCollection(
        creation_date=data.get("test_collection", {}).get("creation_date", ""),
        total_questions=data.get("test_collection", {}).get("total_questions", 0),
        topics_count=data.get("test_collection", {}).get("topics_count", 0),
        topics=topics,
    )


def generate_markdown_report(report: EffectivenessReport) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Markdown –æ—Ç—á–µ—Ç —Å —Ç–∞–±–ª–∏—Ü–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    md_lines = [
        "# –û—Ç—á–µ—Ç –æ–± —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—Ç–æ—Ä–∏—á–Ω–æ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏",
        f"\n**–î–∞—Ç–∞:** {report.report_date}",
        "\n## –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        f"- **–í—Å–µ–≥–æ –ø—Ä–æ–≤–µ—Ä–æ–∫:** {report.overall_metrics.total_verifications}",
        f"- **–°–æ–≥–ª–∞—Å–∏–µ –ø—Ä–æ–≤–µ—Ä–æ–∫:** {report.overall_metrics.agreement_count} ({report.overall_metrics.agreement_rate:.1f}%)",
        f"- **–†–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è:** {report.overall_metrics.disagreement_count} ({100 - report.overall_metrics.agreement_rate:.1f}%)",
        "\n### üéØ –¢–æ—á–Ω–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —ç—Ç–∞–ª–æ–Ω–∞\n",
        f"- **–¢–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–≤–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏:** {report.overall_metrics.primary_accuracy:.1f}%",
        f"- **–¢–æ—á–Ω–æ—Å—Ç—å –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏:** {report.overall_metrics.secondary_accuracy:.1f}%",
        f"- **–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏:** {report.overall_metrics.improvement_rate:+.1f}%",
    ]

    # –û—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    md_lines.append("\n## –í—ã–≤–æ–¥—ã –æ–± —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n")
    if report.overall_metrics.improvement_rate > 5:
        md_lines.append(
            "‚úÖ **–í—ã—Å–æ–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**: –í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å (>5%)"
        )
    elif report.overall_metrics.improvement_rate > 0:
        md_lines.append(
            "‚ö†Ô∏è **–£–º–µ—Ä–µ–Ω–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**: –í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–µ—Ç –Ω–µ–±–æ–ª—å—à–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ"
        )
    else:
        md_lines.append("‚ùå **–ù–∏–∑–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**: –í—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ —É–ª—É—á—à–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")

    # –¢–∞–±–ª–∏—Ü–∞ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    md_lines.append("\n## –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º\n")
    md_lines.append(
        "| ID | –¢–æ–ø–∏–∫ | –°–ª–æ–∂–Ω–æ—Å—Ç—å | –≠—Ç–∞–ª–æ–Ω | –ü–µ—Ä–≤–∏—á–Ω–∞—è | –í—Ç–æ—Ä–∏—á–Ω–∞—è | –°–æ–≥–ª–∞—Å–∏–µ | –°—Ç–∞—Ç—É—Å |"
    )
    md_lines.append(
        "|:--:|:------|:---------:|:------:|:---------:|:---------:|:--------:|:------:|"
    )

    for v in report.verifications:
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        q_id = v.question_id
        topic = v.topic[:20] + "..." if len(v.topic) > 20 else v.topic
        difficulty = {"easy": "–õ–µ–≥–∫–æ", "medium": "–°—Ä–µ–¥–Ω–µ", "hard": "–°–ª–æ–∂–Ω–æ"}.get(
            v.difficulty, v.difficulty
        )

        # –≠–º–æ–¥–∑–∏ –¥–ª—è –±—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        ground_truth_emoji = "‚úì" if v.ground_truth else "‚úó"
        primary_emoji = "‚úì" if v.primary_evaluation.is_correct else "‚úó"
        secondary_emoji = "‚úì" if v.secondary_evaluation.is_correct else "‚úó"
        agreement_emoji = "‚úì" if v.secondary_evaluation.agree_with_primary else "‚úó"

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
        if v.secondary_evaluation.is_correct == v.ground_truth:
            if v.primary_evaluation.is_correct == v.ground_truth:
                status = "üü¢"  # –û–±–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ
            else:
                status = "üü°"  # –í—Ç–æ—Ä–∏—á–Ω–∞—è –∏—Å–ø—Ä–∞–≤–∏–ª–∞ –æ—à–∏–±–∫—É
        elif v.primary_evaluation.is_correct == v.ground_truth:
            status = "üî¥"  # –í—Ç–æ—Ä–∏—á–Ω–∞—è —É—Ö—É–¥—à–∏–ª–∞
        else:
            status = "üî¥"  # –û–±–µ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ

        md_lines.append(
            f"| {q_id} | {topic} | {difficulty} | {ground_truth_emoji} | "
            f"{primary_emoji} | {secondary_emoji} | {agreement_emoji} | {status} |"
        )

    # –õ–µ–≥–µ–Ω–¥–∞
    md_lines.append("\n### –õ–µ–≥–µ–Ω–¥–∞\n")
    md_lines.append("- **–≠—Ç–∞–ª–æ–Ω**: –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º")
    md_lines.append("- **–ü–µ—Ä–≤–∏—á–Ω–∞—è/–í—Ç–æ—Ä–∏—á–Ω–∞—è**: –æ—Ü–µ–Ω–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (‚úì = –ø—Ä–∞–≤–∏–ª—å–Ω–æ, ‚úó = –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ)")
    md_lines.append("- **–°–æ–≥–ª–∞—Å–∏–µ**: —Å–æ–≥–ª–∞—Å–Ω–∞ –ª–∏ –≤—Ç–æ—Ä–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –ø–µ—Ä–≤–∏—á–Ω–æ–π")
    md_lines.append(
        "- **–°—Ç–∞—Ç—É—Å**: üü¢ = –≤—Ç–æ—Ä–∏—á–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞, üü° = –≤—Ç–æ—Ä–∏—á–Ω–∞—è –∏—Å–ø—Ä–∞–≤–∏–ª–∞, üî¥ = –æ—à–∏–±–∫–∞"
    )

    return "\n".join(md_lines)


async def main(args: argparse.Namespace) -> None:
    print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    test_collection = load_test_collection_from_file(args.test_data)

    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {test_collection.total_questions} –≤–æ–ø—Ä–æ—Å–æ–≤\n")
    print("‚öôÔ∏è –ù–∞—á–∏–Ω–∞–µ–º –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é...")

    try:
        verifications = await process_verifications(test_collection, args.language)
        print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(verifications)} –≤–æ–ø—Ä–æ—Å–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    overall_metrics = calculate_metrics(verifications)
    report = EffectivenessReport(
        report_date=datetime.now().isoformat(),
        overall_metrics=overall_metrics,
        verifications=verifications,
    )

    markdown = generate_markdown_report(report)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    output_path = Path(args.output)
    output_path.write_text(markdown, encoding="utf-8")
    print(f"\nüìù –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {args.output}")

    # –í—ã–≤–æ–¥–∏–º –∫–ª—é—á–µ–≤—É—é –º–µ—Ç—Ä–∏–∫—É
    print(f"\nüéØ –£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç –≤—Ç–æ—Ä–∏—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏: {overall_metrics.improvement_rate:+.1f}%")
    print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–≤–∏—á–Ω–æ–π: {overall_metrics.primary_accuracy:.1f}%")
    print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –≤—Ç–æ—Ä–∏—á–Ω–æ–π: {overall_metrics.secondary_accuracy:.1f}%")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--test-data", default="test_data_updated.json")
    parser.add_argument("--language", default="ru")
    parser.add_argument("--output", default="effectiveness_report.md")
    args = parser.parse_args()

    asyncio.run(main(args))
